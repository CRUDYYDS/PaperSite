{
  "papers": [
    {
      "id": "mc4yjuwc0o8nu",
      "title": "Language Models are Few-Shot Learners (GPT3)",
      "authors": [
        "OpenAI"
      ],
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\nin architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions – something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\nwith tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe find that GPT-3 can generate samples of news articles which human evaluators have difficulty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n",
      "keywords": [
        "GPT3"
      ],
      "category": "人工智能",
      "publishDate": "2025-06-20",
      "fileName": "GPT3.pdf",
      "fileSize": 6768044,
      "repository": "master",
      "filePath": "papers/2025/ai/1750432891592_language_models_are_few_shot_learners__gpt3_.pdf",
      "downloadCount": 0,
      "createdAt": "2025-06-20T15:21:36.300Z"
    },
    {
      "id": "mc4ylu7p27bl0",
      "title": "The Llama 3 Herd of Models",
      "authors": [
        "Llama Team",
        "AI @ Meta1"
      ],
      "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a\nnew set of foundation models, called Llama 3. It is a herd of language models that natively support\nmultilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with\n405B parameters and a context window of up to 128K tokens. This paper presents an extensive\nempirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language\nmodels such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and\npost-trained versions of the 405B parameter language model and our Llama Guard 3 model for input\nand output safety. The paper also presents the results of experiments in which we integrate image,\nvideo, and speech capabilities into Llama 3 via a compositional approach. We observe this approach\nperforms competitively with the state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under development.",
      "keywords": [
        "llama3"
      ],
      "category": "人工智能",
      "publishDate": "2025-06-20",
      "fileName": "LLama3.pdf",
      "fileSize": 9833173,
      "repository": "master",
      "filePath": "papers/2025/ai/1750432983606_the_llama_3_herd_of_models.pdf",
      "downloadCount": 0,
      "createdAt": "2025-06-20T15:23:08.725Z"
    },
    {
      "id": "mc4zlmfmc4lsn",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming",
        "He Xiangyu",
        "Zhang Shaoqing",
        "Ren Jian",
        "Sun"
      ],
      "abstract": "Deeper neural networks are more difficult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers—8×\ndeeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1\n, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n",
      "keywords": [
        "Resnet"
      ],
      "category": "人工智能",
      "publishDate": "2025-06-20",
      "fileName": "resnet.pdf",
      "fileSize": 819383,
      "repository": "master",
      "filePath": "papers/2025/ai/1750434655428_deep_residual_learning_for_image_recognition.pdf",
      "downloadCount": 0,
      "createdAt": "2025-06-20T15:50:58.258Z"
    }
  ],
  "metadata": {
    "totalCount": 3,
    "lastUpdated": "2025-06-20T15:50:58.924Z",
    "categories": {
      "人工智能": 3
    }
  }
}