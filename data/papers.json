{
  "papers": [
    {
      "id": "mc4yjuwc0o8nu",
      "title": "Language Models are Few-Shot Learners (GPT3)",
      "authors": [
        "OpenAI"
      ],
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\nin architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions – something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\nwith tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe find that GPT-3 can generate samples of news articles which human evaluators have difficulty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n",
      "keywords": [
        "GPT3"
      ],
      "category": "人工智能",
      "publishDate": "2025-06-20",
      "fileName": "GPT3.pdf",
      "fileSize": 6768044,
      "repository": "master",
      "filePath": "papers/2025/ai/1750432891592_language_models_are_few_shot_learners__gpt3_.pdf",
      "downloadCount": 0,
      "createdAt": "2025-06-20T15:21:36.300Z"
    },
    {
      "id": "mc4ylu7p27bl0",
      "title": "The Llama 3 Herd of Models",
      "authors": [
        "Llama Team",
        "AI @ Meta1"
      ],
      "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a\nnew set of foundation models, called Llama 3. It is a herd of language models that natively support\nmultilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with\n405B parameters and a context window of up to 128K tokens. This paper presents an extensive\nempirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language\nmodels such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and\npost-trained versions of the 405B parameter language model and our Llama Guard 3 model for input\nand output safety. The paper also presents the results of experiments in which we integrate image,\nvideo, and speech capabilities into Llama 3 via a compositional approach. We observe this approach\nperforms competitively with the state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under development.",
      "keywords": [
        "llama3"
      ],
      "category": "人工智能",
      "publishDate": "2025-06-20",
      "fileName": "LLama3.pdf",
      "fileSize": 9833173,
      "repository": "master",
      "filePath": "papers/2025/ai/1750432983606_the_llama_3_herd_of_models.pdf",
      "downloadCount": 0,
      "createdAt": "2025-06-20T15:23:08.725Z"
    }
  ],
  "metadata": {
    "totalCount": 4,
    "lastUpdated": "2025-06-20T15:23:37.577Z",
    "categories": {
      "其他": 1,
      "人工智能": 3
    }
  }
}