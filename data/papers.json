{
  "papers": [
    {
      "id": "mc3meiuvlzhop",
      "title": "BERT",
      "filePath": "papers/2025/ai/1750352023381_bert.pdf",
      "fileUrl": "papers/2025/ai/1750352023381_bert.pdf"
    },
    {
      "id": "mc4yjuwc0o8nu",
      "title": "Language Models are Few-Shot Learners (GPT3)",
      "authors": [
        "OpenAI"
      ],
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\nin architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions – something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\nwith tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe find that GPT-3 can generate samples of news articles which human evaluators have difficulty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n",
      "keywords": [
        "GPT3"
      ],
      "category": "人工智能",
      "publishDate": "2025-06-20",
      "fileName": "GPT3.pdf",
      "fileSize": 6768044,
      "repository": "master",
      "filePath": "papers/2025/ai/1750432891592_language_models_are_few_shot_learners__gpt3_.pdf",
      "downloadCount": 0,
      "createdAt": "2025-06-20T15:21:36.300Z"
    }
  ],
  "metadata": {
    "totalCount": 2,
    "lastUpdated": "2025-06-20T15:21:36.979Z",
    "categories": {
      "其他": 1,
      "人工智能": 1
    }
  }
}